{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/my_env/lib/python3.9/site-packages (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Length: 1112350\n",
      "Unique Characters: 80\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "## Reading and processing text\n",
    "with open('1268-0.txt', 'r', encoding=\"utf8\") as fp:\n",
    "    text=fp.read()\n",
    "    \n",
    "start_indx = text.find('THE MYSTERIOUS ISLAND')\n",
    "end_indx = text.find('End of the Project Gutenberg')\n",
    "\n",
    "text = text[start_indx:end_indx]\n",
    "char_set = set(text)\n",
    "print('Total Length:', len(text))\n",
    "print('Unique Characters:', len(char_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text encoded shape:  (1112350,)\n",
      "THE MYSTERIOUS       == Encoding ==>  [44 32 29  1 37 48 43 44 29 42 33 39 45 43  1]\n",
      "[33 43 36 25 38 28]  == Reverse  ==>  ISLAND\n"
     ]
    }
   ],
   "source": [
    "chars_sorted = sorted(char_set)\n",
    "char2int = {ch:i for i,ch in enumerate(chars_sorted)}\n",
    "char_array = np.array(chars_sorted)\n",
    "\n",
    "text_encoded = np.array(\n",
    "    [char2int[ch] for ch in text],\n",
    "    dtype=np.int32)\n",
    "\n",
    "print('Text encoded shape: ', text_encoded.shape)\n",
    "\n",
    "print(text[:15], '     == Encoding ==> ', text_encoded[:15])\n",
    "print(text_encoded[15:21], ' == Reverse  ==> ', ''.join(char_array[text_encoded[15:21]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 -> T\n",
      "32 -> H\n",
      "29 -> E\n",
      "1 ->  \n",
      "37 -> M\n"
     ]
    }
   ],
   "source": [
    "for ex in text_encoded[:5]:\n",
    "    print('{} -> {}'.format(ex, char_array[ex]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[44 32 29  1 37 48 43 44 29 42 33 39 45 43  1 33 43 36 25 38 28  1  6  6\n",
      "  6  0  0  0  0  0 40 67 64 53 70 52 54 53  1 51]  ->  74\n",
      "'THE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nProduced b'  ->  'y'\n"
     ]
    }
   ],
   "source": [
    "seq_length = 40\n",
    "chunk_size = seq_length + 1\n",
    "\n",
    "text_chunks = [text_encoded[i:i+chunk_size] \n",
    "               for i in range(len(text_encoded)-chunk_size+1)] \n",
    "\n",
    "## inspection:\n",
    "for seq in text_chunks[:1]:\n",
    "    input_seq = seq[:seq_length]\n",
    "    target = seq[seq_length] \n",
    "    print(input_seq, ' -> ', target)\n",
    "    print(repr(''.join(char_array[input_seq])), \n",
    "          ' -> ', repr(''.join(char_array[target])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/anaconda3/envs/my_env/lib/python3.9/site-packages (2.7.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/my_env/lib/python3.9/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/envs/my_env/lib/python3.9/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/envs/my_env/lib/python3.9/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/my_env/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/my_env/lib/python3.9/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/envs/my_env/lib/python3.9/site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/my_env/lib/python3.9/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/my_env/lib/python3.9/site-packages (from jinja2->torch) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pn/m2nfshws0wd37gvf2z0f6v040000gn/T/ipykernel_17748/254275106.py:16: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:257.)\n",
      "  seq_dataset = TextDataset(torch.tensor(text_chunks, dtype=torch.long))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text_chunks):\n",
    "        self.text_chunks = text_chunks\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_chunks)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text_chunk = self.text_chunks[idx]\n",
    "        return text_chunk[:-1].long(), text_chunk[1:].long()\n",
    "    \n",
    "seq_dataset = TextDataset(torch.tensor(text_chunks, dtype=torch.long))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS device\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    print(\"Using MPS device\")\n",
    "else:\n",
    "    print(\"MPS (Metal Performance Shaders) not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Input (x): 'THE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nProduced b'\n",
      "Target (y): 'HE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nProduced by'\n",
      "\n",
      " Input (x): 'HE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nProduced by'\n",
      "Target (y): 'E MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nProduced by '\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, (seq, target) in enumerate(seq_dataset):\n",
    "    print(' Input (x):', repr(''.join(char_array[seq])))\n",
    "    print('Target (y):', repr(''.join(char_array[target])))\n",
    "    print()\n",
    "    if i == 1:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    " \n",
    "batch_size = 32\n",
    "\n",
    "torch.manual_seed(1)\n",
    "seq_dl = DataLoader(seq_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn_hidden_size = rnn_hidden_size\n",
    "        self.lstm = nn.LSTM(embed_dim, rnn_hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(rnn_hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        out = self.embedding(x)\n",
    "        out, (hidden, cell) = self.lstm(out, (hidden, cell))\n",
    "        out = self.fc(out)\n",
    "        return out, hidden, cell\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(1, batch_size, self.rnn_hidden_size).to(device)\n",
    "        cell = torch.zeros(1, batch_size, self.rnn_hidden_size).to(device)\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn_hidden_size = rnn_hidden_size\n",
    "        self.rnn = nn.RNN(embed_dim, rnn_hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(rnn_hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        out = self.embedding(x)\n",
    "        out, hidden = self.rnn(out, hidden)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(1, batch_size, self.rnn_hidden_size).to(device)\n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TwoLayerRNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn_hidden_size = rnn_hidden_size\n",
    "        #  Add num_layers=2 for 2 RNN layers\n",
    "        self.rnn = nn.RNN(embed_dim, rnn_hidden_size, num_layers=2, batch_first=True)\n",
    "        self.fc = nn.Linear(rnn_hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        out = self.embedding(x)\n",
    "        out, hidden = self.rnn(out, hidden)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        #  Now hidden state needs 2 layers\n",
    "        hidden = torch.zeros(2, batch_size, self.rnn_hidden_size).to(device)\n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "✅ Models saved!\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "vocab_size = len(char_array)\n",
    "embed_dim = 256\n",
    "rnn_hidden_size = 256\n",
    "\n",
    "lstm_model = LSTMModel(vocab_size, embed_dim, rnn_hidden_size).to(device)\n",
    "rnn_model = RNNModel(vocab_size, embed_dim, rnn_hidden_size).to(device)\n",
    "two_rnn_model = TwoLayerRNNModel(vocab_size, embed_dim, rnn_hidden_size).to(device)\n",
    "\n",
    "# Save models\n",
    "torch.save(lstm_model.state_dict(), 'lstm_model.pth')\n",
    "torch.save(rnn_model.state_dict(), 'rnn_model.pth')\n",
    "torch.save(two_rnn_model.state_dict(), 'two_rnn_model.pth')\n",
    "\n",
    "print(\"✅ Models saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training Function LSTM AND RNN ---\n",
    "def train_model(model, model_type='lstm', num_epochs=3):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for batch in seq_dl:  # YOUR DataLoader here\n",
    "            input_seq, target_seq = batch\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "\n",
    "            batch_size = input_seq.size(0)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if model_type == 'lstm':\n",
    "                hidden, cell = model.init_hidden(batch_size)\n",
    "                outputs, hidden, cell = model(input_seq, hidden, cell)\n",
    "            else:  # RNN\n",
    "                hidden = model.init_hidden(batch_size)\n",
    "                outputs, hidden = model(input_seq, hidden)\n",
    "\n",
    "            # Reshape outputs and targets to match CrossEntropyLoss expectation\n",
    "            outputs = outputs.reshape(-1, vocab_size)\n",
    "            target_seq = target_seq.reshape(-1)\n",
    "\n",
    "            loss = criterion(outputs, target_seq)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == target_seq).sum().item()\n",
    "            total += target_seq.numel()\n",
    "\n",
    "        accuracy = correct / total\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}, Accuracy: {accuracy*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training Function for TwoLayerRNNModel ---\n",
    "def train_two_rnn_model(model, num_epochs=3):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for batch in seq_dl:  # YOUR DataLoader here\n",
    "            input_seq, target_seq = batch\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "\n",
    "            batch_size = input_seq.size(0)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 🌟 For TwoLayerRNNModel\n",
    "            hidden = model.init_hidden(batch_size)\n",
    "            outputs, hidden = model(input_seq, hidden)\n",
    "\n",
    "            # Reshape outputs and targets to match CrossEntropyLoss expectation\n",
    "            outputs = outputs.reshape(-1, vocab_size)\n",
    "            target_seq = target_seq.reshape(-1)\n",
    "\n",
    "            loss = criterion(outputs, target_seq)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == target_seq).sum().item()\n",
    "            total += target_seq.numel()\n",
    "\n",
    "        accuracy = correct / total\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}, Accuracy: {accuracy*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LSTM and RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load and Train LSTM Model ---\n",
    "print(\"\\n🔵 Training LSTM Model\")\n",
    "lstm_loaded = LSTMModel(vocab_size, embed_dim, rnn_hidden_size).to(device)\n",
    "lstm_loaded.load_state_dict(torch.load('lstm_model.pth'))\n",
    "\n",
    "train_model(lstm_loaded, model_type='lstm')\n",
    "\n",
    "# --- Load and Train RNN Model ---\n",
    "print(\"\\n🟢 Training RNN Model\")\n",
    "rnn_loaded = RNNModel(vocab_size, embed_dim, rnn_hidden_size).to(device)\n",
    "rnn_loaded.load_state_dict(torch.load('rnn_model.pth'))\n",
    "\n",
    "train_model(rnn_loaded, model_type='rnn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train 2RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🟣 Training TwoLayer RNN Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brapo\\AppData\\Local\\Temp\\ipykernel_5200\\3631671914.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  two_rnn_loaded.load_state_dict(torch.load('two_rnn_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Loss: 41971.4966, Accuracy: 62.89%\n",
      "Epoch [2/3], Loss: 39707.5141, Accuracy: 64.54%\n",
      "Epoch [3/3], Loss: 39771.8634, Accuracy: 64.46%\n"
     ]
    }
   ],
   "source": [
    "# --- Load and Train TwoLayer RNN Model ---\n",
    "print(\"\\n🟣 Training TwoLayer RNN Model\")\n",
    "two_rnn_loaded = TwoLayerRNNModel(vocab_size, embed_dim, rnn_hidden_size).to(device)\n",
    "two_rnn_loaded.load_state_dict(torch.load('two_rnn_model.pth'))\n",
    "\n",
    "train_two_rnn_model(two_rnn_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, starting_str, len_generated_text=500, scale_factor=1.0):\n",
    "    encoded_input = torch.tensor([char2int[s] for s in starting_str]).to(device)\n",
    "    encoded_input = torch.reshape(encoded_input, (1, -1))\n",
    "\n",
    "    generated_str = starting_str\n",
    "\n",
    "    # Check if model is LSTM (has 2 hidden states) or RNN (has 1 hidden state)\n",
    "    hidden = model.init_hidden(1)\n",
    "    if isinstance(hidden, tuple):\n",
    "        hidden, cell = hidden\n",
    "    else:\n",
    "        cell = None  # no cell for RNN\n",
    "\n",
    "    hidden = hidden.to(device)\n",
    "    if cell is not None:\n",
    "        cell = cell.to(device)\n",
    "\n",
    "    for c in range(len(starting_str) - 1):\n",
    "        if cell is not None:\n",
    "            _, hidden, cell = model(encoded_input[:, c].view(1, 1), hidden, cell)\n",
    "        else:\n",
    "            _, hidden = model(encoded_input[:, c].view(1, 1), hidden)\n",
    "\n",
    "    last_char = encoded_input[:, -1]\n",
    "    for i in range(len_generated_text):\n",
    "        if cell is not None:\n",
    "            logits, hidden, cell = model(last_char.view(1, 1), hidden, cell)\n",
    "        else:\n",
    "            logits, hidden = model(last_char.view(1, 1), hidden)\n",
    "\n",
    "        logits = torch.squeeze(logits, 0)\n",
    "        scaled_logits = logits * scale_factor\n",
    "        m = torch.distributions.Categorical(logits=scaled_logits)\n",
    "        last_char = m.sample()\n",
    "\n",
    "        generated_str += str(char_array[last_char])\n",
    "\n",
    "    return generated_str\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔵 LSTM Generated Texts:\n",
      "\n",
      "Sample 1:\n",
      "The island! I should know the twild it markles.”\n",
      "\n",
      "River,\n",
      "which belonged to the world, when a very nine discoveries at the top of this act again. While Neb,” and evidently found lay to be followed. Pencroft soon as\n",
      "the height of smoke against eleven days. A finall side, phoys alone in the first were managed to return to see at the foxes, fell less proportion Britgle will have to prevent a chance of a slight of it frost.”\n",
      "\n",
      "“We are unhairs. They must\n",
      "find him from extinction. He gave the\n",
      "passage, carried the\n",
      "\n",
      "Sample 2:\n",
      "The island in breadth, and youn leave any wise and\n",
      "devoiced that he would have a greater, a fiber or even a new which I have no hour, he desired, the sailor’s sapiliting project, when Captain Nemo’s arms of\n",
      "little rescaping by the hunt, found themethout the castaways could not be tons of\n",
      "his idea. “At sufficient shore excursions, which have returned to the northern zinc inevoutive\n",
      "and perfectly made it disappeared. He could soon see the quantity of sight, produced in the globe\n",
      "car, as they had spared thei\n",
      "\n",
      "🟢 RNN Generated Texts:\n",
      "\n",
      "Sample 1:\n",
      "The island!”\n",
      "\n",
      "“It is the colonists had confided the citate of Scusiously exposed this was a space less than you two above the\n",
      "corral, more than these new barrels.\n",
      "\n",
      "The corral, and a house that a hunday that the lowever we showled him the shore, and, yet confined to\n",
      "be\n",
      "uncand the tide of you,” Cyrus Harding and Herbert. Reanths which had been rest, agreed sleds distance escaped the sea-hape of the sailor, Herbert?”\n",
      "\n",
      "“Yes,” replied Gideon Spilits were here! Since a couples\n",
      "to after it not broken, within,” a\n",
      "\n",
      "Sample 2:\n",
      "The island. An except the plateau to go one of five him, and the rest crossudened the forests of seasts are is that of the reasorance\n",
      "dyy, for the lips, at the stranged the manufactured. It would not almost was impossible to do! \n",
      "\n",
      "\n",
      "Chapter 9\n",
      "\n",
      "The fater reporters; either thought. It was\n",
      "without rightles, cappe\n",
      "on the tempest cried Linter in a ramps each otherday him, the\n",
      "plateau, though it was\n",
      "carmer\n",
      "powder, it was on the island in the sailor, “or of\n",
      "land,\n",
      "therefore, food on animals over them! And Pencroft\n",
      "\n",
      "🟣 Two-Layer RNN Generated Texts:\n",
      "\n",
      "Sample 1:\n",
      "The island, and the next day to the expeditory. It was quite saying age in no spot, exterior of\n",
      "the bago, which furnish,” added the interior, it is never been able that he would\n",
      "ever cantained by Granite House in its\n",
      "valvey, which, and for a turts and years\n",
      "without such a peppersely, fart-luces, an echunged a long time had\n",
      "absenging a vessels or Imployed neards were, which getting out of the Grant-curvely replied the shore.\n",
      "\n",
      "In fact, I would regards; you are you know like. The unhappy,” resu thought a boa\n",
      "\n",
      "Sample 2:\n",
      "The island.”\n",
      "\n",
      "“And it was importantly towards the letter. If this unknown silended, over-contractic ray reply of smoke above the rivally had nothing, then reached\n",
      "their fing with their pointed ninetraier is not, im to throw any prisoned in fired the\n",
      "beasts--sharp creature and its flowing on the circose glances. All the end, it\n",
      "would bear what immediately, better, he gave these degree it to be feared. This time\n",
      "has an engineer, “but it might get about at\n",
      "the\n",
      "horizon,” said the sailor. “Go-terrs!”\n",
      "\n",
      "“Well.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Move models to evaluation mode\n",
    "lstm_loaded.eval()\n",
    "rnn_loaded.eval()\n",
    "two_rnn_loaded.eval()\n",
    "\n",
    "# Define how many samples you want\n",
    "num_samples = 2\n",
    "\n",
    "print(\"\\n🔵 LSTM Generated Texts:\")\n",
    "for i in range(num_samples):\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(sample(lstm_loaded, starting_str='The island'))\n",
    "\n",
    "print(\"\\n🟢 RNN Generated Texts:\")\n",
    "for i in range(num_samples):\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(sample(rnn_loaded, starting_str='The island'))\n",
    "\n",
    "print(\"\\n🟣 Two-Layer RNN Generated Texts:\")\n",
    "for i in range(num_samples):\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(sample(two_rnn_loaded, starting_str='The island'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔵 LSTM Generated Texts (Scale Factor 1.2):\n",
      "\n",
      "Sample 1:\n",
      "The island was bottle, the Southerner\n",
      "distance, he had only to wait himself in its turn. To the oars of the inexplicable face. The wool had broken so happies marched the channel, which was heard, he turned themselves a prison-cure, carried\n",
      "all important change!” he cried, “what they would have been seen that in the morning. This lay to considerable suit to come to the scarcely the lava, the current late as the engineer and\n",
      "it was more than unimportant from it.\n",
      "\n",
      "“Yes, and if the balloon and Neb was to esca\n",
      "\n",
      "Sample 2:\n",
      "The island showed that the beach they would ascertain that a fibround seals. As American coast. The wall of the hummer that the trade still fired, and, that he had formed a single\n",
      "numerous\n",
      "breath, at a similar excursions, or it is your dimension made, for the most faithful day only some days to set out the south being knew to his country.\n",
      "\n",
      "The convicts were near the species arriving on the island, the precision.\n",
      "They died his ship in the second\n",
      "temperature with the signal to send at difrculate\n",
      "and force.\n",
      "\n",
      "\n",
      "🟢 RNN Generated Texts (Scale Factor 1.5):\n",
      "\n",
      "Sample 1:\n",
      "The island, and now heavy by powder, and which he has proved on the engineer wished to get him of submerful of the movement was heard happiness from besides, as if we have not the engineer was discovered as seized any one has reached the settlers described Pencroft, “and the world, he could not find it a long supply of a hundred feet and formed by the surprised in the feed neither the engineer mouth of the rid been heard which the sea,\n",
      "and made a supplish the first stream which is come\n",
      "and gazed at the co\n",
      "\n",
      "Sample 2:\n",
      "The island, and before, and the place of lava, the balloon and the water’s executted in the began to fire, and this place the channel. It is a man should be on the sailor when that parts of the lake,\n",
      "ended it and his companions were in the corral, and Pencroft looked the glood of branches.\n",
      "\n",
      "It was a situation of a man and Jup and here the netween the settlers terminated, and distance of the house, and which were full was poing them himself of the condition in the level\n",
      "of the\n",
      "part of the stones which the \n",
      "\n",
      "🟣 Two-Layer RNN Generated Texts (Scale Factor 1.7):\n",
      "\n",
      "Sample 1:\n",
      "The island was not seen in the corral.\n",
      "\n",
      "As to the same to the edge of the corral. The engineer one of these branches are not more than three and-hundred feet a serious, and completed to settle how to begin the sea. But the human being composed, and the captain was extended at the sand, and the colonists. It was experience to the plateau with the sea-bled with gradually belonged at this masts. The arranged between them to accompanions, and the wood, and the oil--and the reporter\n",
      "arrived one of the rocks. I\n",
      "\n",
      "Sample 2:\n",
      "The island, but he did not think of the forest--”\n",
      "\n",
      "“What was the bittle vessel proposed to accomplished by the loging and waters, well, which was of different coast, without leaving a single granited in the stranger of the affairs,” answered the engineer, “we are since you see if this latitude in the fared to the situation of the mouth of the sea, and the articles to return to the bottom of the light of the sky. But it was nearly besides the salton with the north of the southern coast was not a sharp retu\n"
     ]
    }
   ],
   "source": [
    "# Move models to evaluation mode\n",
    "lstm_loaded.eval()\n",
    "rnn_loaded.eval()\n",
    "two_rnn_loaded.eval()\n",
    "\n",
    "# Define how many samples you want\n",
    "num_samples = 2\n",
    "\n",
    "print(\"\\n🔵 LSTM Generated Texts (Scale Factor 1.2):\")\n",
    "for i in range(num_samples):\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(sample(lstm_loaded, starting_str='The island', scale_factor=1.2))\n",
    "\n",
    "print(\"\\n🟢 RNN Generated Texts (Scale Factor 1.5):\")\n",
    "for i in range(num_samples):\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(sample(rnn_loaded, starting_str='The island', scale_factor=1.5))\n",
    "\n",
    "print(\"\\n🟣 Two-Layer RNN Generated Texts (Scale Factor 1.7):\")\n",
    "for i in range(num_samples):\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(sample(two_rnn_loaded, starting_str='The island', scale_factor=1.7))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
